{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import torch","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install segmentation_models_pytorch","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting segmentation_models_pytorch\n  Downloading segmentation_models_pytorch-0.1.0-py3-none-any.whl (42 kB)\n\u001b[K     |████████████████████████████████| 42 kB 228 kB/s eta 0:00:011\n\u001b[?25hRequirement already satisfied: torchvision>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from segmentation_models_pytorch) (0.6.0a0+35d732a)\nCollecting pretrainedmodels==0.7.4\n  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n\u001b[K     |████████████████████████████████| 58 kB 1.5 MB/s eta 0:00:011\n\u001b[?25hCollecting efficientnet-pytorch>=0.5.1\n  Downloading efficientnet_pytorch-0.7.0.tar.gz (20 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (1.18.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (1.5.1)\nRequirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.7/site-packages (from torchvision>=0.3.0->segmentation_models_pytorch) (7.2.0)\nRequirement already satisfied: munch in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (2.5.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from pretrainedmodels==0.7.4->segmentation_models_pytorch) (4.45.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch->torchvision>=0.3.0->segmentation_models_pytorch) (0.18.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from munch->pretrainedmodels==0.7.4->segmentation_models_pytorch) (1.14.0)\nBuilding wheels for collected packages: pretrainedmodels, efficientnet-pytorch\n  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60962 sha256=1bd8c9b979338e9223284cc3f723433b9f4321547f5187203921c9eaae829b31\n  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.7.0-py3-none-any.whl size=16035 sha256=062133c28c605c7ab8445c457fe963f1a9757651f729adaa8c82d4445a46d5a8\n  Stored in directory: /root/.cache/pip/wheels/b7/cc/0d/41d384b0071c6f46e542aded5f8571700ace4f1eb3f1591c29\nSuccessfully built pretrainedmodels efficientnet-pytorch\nInstalling collected packages: pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\nSuccessfully installed efficientnet-pytorch-0.7.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.1.0\n\u001b[33mWARNING: You are using pip version 20.2.1; however, version 20.2.2 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"# MAKE SUBMIT.CSV"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport torch\nimport os\nimport cv2\nimport segmentation_models_pytorch as smp\nfrom tqdm import tqdm_notebook as tqdm\n\ndef test_generator(transforms, path_to_images='test_images'):\n    images_name = os.listdir(path_to_images)\n\n    for name in images_name:\n        image = cv2.imread(os.path.join(path_to_images, name))\n        image = transforms(image=image)['image']\n        yield image, name\n\n\ndef make_submission(\n        model,\n        path_to_images='test_images',\n        device=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu'),\n        threshold=0.5, min_size=[500, 500, 500, 500]):\n\n    model.to(device)\n    model.eval()\n    transforms = get_transforms(phase='val')\n    test_images = test_generator(transforms, path_to_images)\n    result = pd.DataFrame(columns=['ImageId', 'EncodedPixels', 'ClassId'])\n    pbar = tqdm(test_images, total=5506)\n    for image, name in test_images:\n        output = model(image.unsqueeze(0).to(device)).cpu().detach().squeeze()\n        pred = predict(output, threshold=threshold, min_size=min_size)\n        for defect in range(4):\n            rle = mask_to_rle(pred[defect])\n            result = result.append({'ImageId': name, 'EncodedPixels': rle, 'ClassId': defect + 1}, ignore_index=True)\n        pbar.update(1)\n    pbar.close()\n    return result\n\nif __name__ == '__main__':\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = smp.Unet('resnet34', encoder_weights=None, classes=4, activation=None)\n    state = torch.load('../input/best-model/model_epoch_14_score_0.9430.pth', map_location=device)\n    model.load_state_dict(state['state_dict'])\n    for i in [0.5, 0.6, 0.7, 0.8]:\n        result = make_submission(model, threshold=i, path_to_images='../input/severstal-steel-defect-detection/test_images')\n        result['ImageId_ClassId'] = result['ImageId'] + '_' + result['ClassId'].apply(str)\n        result[['EncodedPixels','ImageId_ClassId']].to_csv(str(i)+'submission.csv', index=False)\n    \n    result.head()","execution_count":5,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5506.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdcca9e3c0bd4ceea529266f0f41f26c"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:28: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\nPlease use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5506.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0ca9e98cc7e4f2c8cb854ba0dbc1df3"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5506.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afb4965b7d1043d0ad01be7dfa53cc4e"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"HBox(children=(FloatProgress(value=0.0, max=5506.0), HTML(value='')))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49cbf29056e6400fbe4227c98c756a47"}},"metadata":{}},{"output_type":"stream","text":"\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[['EncodedPixels','ImageId_ClassId']].to_csv('submission1.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# UTILS"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport pandas as pd\nimport torch\n\nDATA_DIR = '../input/severstal-steel-defect-detection/train_images'\ndef mask_to_rle(img):\n    '''\n    img: numpy array, 1 -> mask, 0 -> background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\n\ndef rle_to_mask(rle, shape=(1600, 256)):\n    \"\"\"\n    :param rle: run-length as string formated (start length)\n    :param shape: (width,height) of array to return\n    :return: numpy array, 1 - mask, 0 - background\n    \"\"\"\n    runs = np.array([int(x) for x in rle.split()])\n    runs[1::2] += runs[::2]\n    runs -= 1\n    starts, ends = runs[::2], runs[1::2]\n    mask = np.zeros(shape[0] * shape[1])\n    for start, end in zip(starts, ends):\n        mask[start:end] = 1\n    return mask.reshape(shape).T\n\n\ndef show_defects(image, mask, pallet=((249, 192, 12), (0, 185, 241), (114, 0, 218), (249,50,12))):\n\n    for i in range(4):\n        image[0, mask[i] == 1] = 255\n    plt.imshow(image.permute(1, 2, 0))\n    plt.show()\n\n\ndef show_mask_image(image, mask, pallet=((249, 192, 12), (0, 185, 241), (114, 0, 218), (249, 50, 12))):\n    fig, ax = plt.subplots(figsize=(15, 15))\n    image = image.permute(1, 2, 0).numpy()\n    mask = mask.permute(1, 2, 0)\n\n    for ch in range(4):\n        image[mask[:, :, ch] == 1] = pallet[ch]\n    plt.imshow(image)\n\n    plt.show()\n\n\ndef make_mask(name, df):\n    mask = np.zeros((256, 1600, 4), dtype=np.float32)\n    rows = df.loc[name]\n    for defect in range(1, 5):\n        rle = rows[defect]\n        if not pd.isna(rle):\n            encoded = rle_to_mask(rle)\n            mask[:, :, defect - 1] = encoded\n    return mask\n\ndef dice_single_channel(targets, preds, eps=1e-9):\n    batch_size = preds.shape[0]\n    preds = preds.view((batch_size, -1)).float()\n    targets = targets.view((batch_size, -1)).float()\n    dice = (2 * (preds * targets).sum(1) + eps) / (preds.sum(1) + targets.sum(1) + eps)\n    return dice\n\n\ndef mean_dice_score(targets, outputs, threshold=0.5, min_size=[600, 600, 1000, 2000]):\n    batch_size = outputs.shape[0]\n    n_channels = outputs.shape[1]\n    \n    preds = torch.zeros_like(outputs)\n    for i in range(batch_size):\n        preds[i] = predict(output=outputs[i], threshold=threshold, min_size=min_size)\n\n    mean_dice = 0\n    for i in range(n_channels):\n        dice = dice_single_channel(targets[:, i, :, :], preds[:, i, :, :])\n        mean_dice += dice.sum(0) / (n_channels * batch_size)\n    return mean_dice.item()\n\n\ndef pixel_accuracy_score(targets, outputs, threshold=0.5, min_size=[600, 600, 1000, 2000]):\n    preds = torch.zeros_like(outputs)\n    for i in range(outputs.shape[0]):\n        preds[i] = predict(output=outputs[i], threshold=threshold, min_size=min_size)\n    correct = torch.sum((targets == preds)).item()\n    total = outputs.numel()\n    return correct / total\n\n\ndef epoch_metrics(targets, outputs, threshold=0.5):\n    return {'dice': mean_dice_score(targets, outputs, threshold),\n            'pixel_acc': pixel_accuracy_score(targets, outputs, threshold)}\n\n\ndef predict(output, threshold=0.5, min_size=[600, 600, 1000, 2000]):\n    output = output.sigmoid()\n    prediction = torch.zeros_like(output)\n    for i in range(4):\n        pixels = (output[i] > threshold).float().sum()\n        if pixels < min_size[i]:\n            prediction[i] = torch.zeros_like(output[i])\n        else:\n            prediction[i] = (output[i] > threshold).float()\n    return prediction\n\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nimport cv2\nimport os\nimport albumentations as albu\nimport albumentations.pytorch as albu_pytorch\nfrom sklearn.model_selection import train_test_split\n\n\nclass SteelDataset(Dataset):\n    def __init__(self, dataset, phase='train', data_dir=DATA_DIR, image_size=(256, 1600), n_classes=4):\n        self.dataset = dataset\n        self.phase = phase\n        self.dir = data_dir\n        self.transforms = get_transforms(phase=self.phase)\n        self.image_size = image_size\n        self.n_classes = n_classes\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, index):\n        name = self.dataset.iloc[index].name\n        image = cv2.imread(os.path.join(self.dir, name))\n        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        mask = make_mask(name, self.dataset)\n\n        transformed = self.transforms(image=image, mask=mask)\n        image, mask = transformed['image'], transformed['mask'].permute(2, 0 , 1)\n\n        return image, mask\n\n\ndef get_transforms(list_transforms=None, phase='train'):\n    if not list_transforms:\n        list_transforms = []\n\n    if phase == 'train':\n        list_transforms.extend(\n            [\n                albu.RandomBrightnessContrast(p=0.1, brightness_limit=0.1, contrast_limit=0.1),\n                albu.HorizontalFlip(p=0.5),\n                albu.VerticalFlip(p=0.5),\n                # albu.ElasticTransform(p=0.5),\n                # albu.GridDistortion(p=0.5),\n                # albu.OpticalDistortion(p=0.5),\n            ]\n        )\n    list_transforms.extend(\n        [\n            albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n            albu_pytorch.ToTensorV2()\n        ]\n    )\n\n    list_transforms = albu.Compose(list_transforms)\n    return list_transforms\n\n\ndef data_provider(df, batch_size=8, shuffle=True, stratify_by=None):\n\n    if stratify_by:\n        train_df, val_df = train_test_split(df, test_size=0.2,\n                                            stratify=df[stratify_by],\n                                            random_state=42,\n                                            shuffle=shuffle)\n    else:\n        train_df, val_df = train_test_split(df, test_size=0.2,\n                                            random_state=42,\n                                            shuffle=shuffle)\n\n    dataloader = {'train': DataLoader(SteelDataset(train_df, phase='train'), batch_size=batch_size),\n                  'val': DataLoader(SteelDataset(val_df, phase='val'), batch_size=batch_size)}\n\n\n    return dataloader\n","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":" # VALIDATION\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"import json\nimport torch\nfrom torch import nn\nimport segmentation_models_pytorch as smp\nimport numpy as np\nimport pandas as pd\nimport time\nfrom tqdm import tqdm_notebook as tqdm\n\n\nclass Trainer:\n    def __init__(self, model, device, data_frame, batch_size=8, stratify_by=None):\n        self.model = model\n        self.device = device\n        self.model = model.to(device)\n        \n        self.device = device\n        self.batch_size = batch_size\n        \n        self.df = data_frame\n        self.dataloaders = data_provider(self.df, batch_size=self.batch_size, stratify_by=stratify_by)\n        \n        self.metrics = {'dice': mean_dice_score, 'pixel_acc': pixel_accuracy_score}\n        self.metrics_values = {phase: {name: [] for name in self.metrics.keys()}\n                               for phase in ['train', 'val']}\n        \n        self.best_score =np.array([-np.inf for _ in self.metrics.keys()])\n\n        \n    def step(self, threshold, min_size, phase='val'):\n\n        metrics = {name: [] for name in self.metrics.keys()}\n        epoch_metric = {}\n\n        self.model.eval()\n\n        dataloader = self.dataloaders[phase]\n        pbar = tqdm(dataloader, total=len(dataloader))\n\n        for i, (images, targets) in enumerate(dataloader):\n            images, targets = images.to(self.device), targets.to(self.device)\n\n            with torch.set_grad_enabled(phase == 'train'):\n                outputs = self.model(images)\n        \n                for metric in self.metrics.keys():\n                    metrics[metric].append(self.metrics[metric](targets, outputs, threshold, min_size))\n\n                pbar.update(1)\n                \n        pbar.close()\n        \n        for metric in self.metrics.keys():\n            epoch_metric[metric] = np.mean(metrics[metric])\n            self.metrics_values[phase][metric].append(epoch_metric[metric])\n\n        del images, targets, outputs\n        torch.cuda.empty_cache()\n        return epoch_metric\n\n    def train(self):\n        for threshold in tqdm(np.arange(0.3, 0.46, 0.01)):\n        \n            metric = self.step('val')\n            print('|val_metric {}'.format(metric))\n            \n            scores = np.fromiter(metric.values(), dtype=np.float)\n            if (scores[0] > self.best_score[0]).all():\n                print('-' * 10 + 'New optimal threshold found and saved' + '-' * 10)\n                self.best_score = scores\n\n\nif __name__ == '__main__':\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    model = smp.Unet('resnet34', encoder_weights=None, classes=4, activation=None)\n    state = torch.load('../input/best-model/model_epoch_14_score_0.9430.pth', map_location=device)\n    model.load_state_dict(state['state_dict'])\n    \n    df = pd.read_csv('../input/severstal-steel-defect-detection/train.csv')\n    without_def = set(os.listdir('../input/severstal-steel-defect-detection/train_images')) - set(list(df['ImageId']))\n    df = df.pivot(index='ImageId', columns='ClassId', values='EncodedPixels')\n    for i in list(without_def):\n        df.loc[i] = [pd.NA, pd.NA, pd.NA, pd.NA]\n        \n    df['NumDefects'] = df.count(axis=1)\n    \n    model_train = Trainer(\n        model=model,\n        device=device,\n        batch_size=12,\n        data_frame=df,\n        stratify_by='NumDefects'\n    )\n    \n\n    min_size = [600, 600, 1000, 2000]","execution_count":6,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'train_images'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-88032957a2e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/severstal-steel-defect-detection/train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mwithout_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ImageId'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ImageId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ClassId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'EncodedPixels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithout_def\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train_images'"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"best = 0.8903\nfor i in np.arange(0.7, 0.9, 0.01):\n    for j in [[500, 500, 500, 500], [600, 600, 1000, 2000]]:\n        print(i, j)\n        metric = model_train.step(threshold=i, min_size=j)\n        if metric['dice'] > best:\n            best = metric['dice']\n            best_param = (i, j)\n        print(metric)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"best_param","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport pandas as pd\nimport torch\nimport os\nimport cv2\nimport segmentation_models_pytorch as smp\n\nclass ModelCombinet(nn.Module):\n    def __init__(self, models):\n        super(ModelCombinet, self).__init__()\n        self.models = models\n    \n    def __call__(self, x):\n        res = []\n        x = x.cuda()\n        with torch.no_grad():\n            for m in self.models:\n                res.append(m(x))\n        res = torch.stack(res)\n        return torch.mean(res, dim=0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel1 = smp.Unet('resnet34', encoder_weights=None, classes=4, activation=None)\nstate1 = torch.load('../input/best-model/model_epoch_1_score_0.8776.pth', map_location=device)\nmodel1.load_state_dict(state1['state_dict'])\nmodel1.to(device)\n\nmodel2 = smp.Unet('resnet34', encoder_weights=None, classes=4, activation=None)\nstate2 = torch.load('../input/best-model/model_epoch_21_score_0.8714.pth', map_location=device)\nmodel2.load_state_dict(state2['state_dict'])\nmodel2.to(device)    \n\nmodel3 = smp.Unet('resnet18', encoder_weights=None, classes=4, activation=None)\nstate3 = torch.load('../input/best-model/model_epoch_19_score_0.8847 (1).pth', map_location=device)\nmodel3.load_state_dict(state3['state_dict'])\nmodel3.to(device)\n\ndf = pd.read_csv('../input/severstal-steel-defect-detection/train.csv')\ndf = df.pivot(index='ImageId', columns='ClassId', values='EncodedPixels')\ndf['NumDefects'] = df.count(axis=1)\n    \n    \nmodel = ModelCombinet([model1, model2, model3])\n# model_train = Trainer(\n#         model=model,\n#         device=device,\n#         batch_size=12,\n#         data_frame=df,\n#         stratify_by='NumDefects'\n# )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_train.step(threshold=0.38, min_size=[600, 600, 1000, 2000])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"{'dice': 0.8869642785617283, 'pixel_acc': 0.9940975052969796}\n{'dice': 0.8885742255619594, 'pixel_acc': 0.9940522198449998}","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}